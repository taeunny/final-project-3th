# -*- coding: utf-8 -*-
"""DPO_fine-tuning&inference.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FLnCk7YTdUalSDP6IOOMpf0cTz8n2kNS

## DPO
"""

!pip install -q datasets wandb accelerate==0.26.1 peft==0.8.2 bitsandbytes==0.42.0 transformers==4.37.2 trl==0.7.10

import torch
import transformers
from datasets import load_dataset, Dataset

from transformers import TrainerCallback, TrainingArguments, TrainerState, TrainerControl
from transformers.trainer_utils import PREFIX_CHECKPOINT_DIR
from torch.nn import functional as F

from peft import (
    LoraConfig,
    get_peft_model,
    prepare_model_for_kbit_training,
    set_peft_model_state_dict,
    AutoPeftModelForCausalLM
)

from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
from trl import DPOTrainer
import wandb

base_model= "uine/single-practice-fine-tuning-eeve-adapter"
data_path= "uine/single-dpo-practice-dataset"
output_dir= ""  # 수정

# training hyperparams
batch_size= 2
micro_batch_size= 1
gradient_accumulation_steps = batch_size // micro_batch_size
num_epochs= 1
learning_rate= 0.00005
cutoff_len= 4096
val_set_size= 0
lr_scheduler= "cosine"
warmup_ratio= 0.1

# lora hyperparams
lora_r= 256
lora_alpha= 128
lora_dropout= 0.05
# from peft docs: ["q_proj", "k_proj", "v_proj", "o_proj", "fc_in", "fc_out", "wte", "gate_proj", "down_proj", "up_proj"]
lora_target_modules = ["gate_proj", "down_proj", "up_proj"]

# llm hyperparams
train_on_inputs=False # if False, masks out inputs in loss
add_eos_token= False
group_by_length= False  # faster, but produces an odd training loss curve
# wandb params
#wandb_project: str = "",
#wandb_run_name: str = "",
#wandb_watch: str = "",  # options: false | gradients | all
#wandb_log_model: str = "",  # options: false | true
resume_from_checkpoint= None  # either training checkpoint or final adapter
# prompt_template_name= "alpaca"
# NEFTune params
noise_alpha= 5

from huggingface_hub import login
login(token='')  # 수정

# 1. Define policy and reference models
compute_dtype = getattr(torch, 'float16')

quant_config = BitsAndBytesConfig(
load_in_4bit=True,
bnb_4bit_quant_type="nf4",
bnb_4bit_compute_dtype=compute_dtype,
bnb_4bit_use_double_quant=False,
)

model = AutoPeftModelForCausalLM.from_pretrained(base_model,
                                                  quantization_config=quant_config,
                                                  device_map="auto")

model_ref = AutoPeftModelForCausalLM.from_pretrained(base_model,
                                                  quantization_config=quant_config,
                                                  device_map="auto")

tokenizer = AutoTokenizer.from_pretrained("yanolja/EEVE-Korean-Instruct-10.8B-v1.0", trust_remote_code=True)

print(type(model))
print(model)
print("length of tokenizer:",len(tokenizer))

bos = tokenizer.bos_token_id
eos = tokenizer.eos_token_id
pad = tokenizer.pad_token_id
print("pre-trained model's BOS EOS and PAD token id:",bos,eos,pad," => It should be 1 2 None")

tokenizer.pad_token_id = 0  # unk. we want this to be different from the eos token
tokenizer.padding_side = "right"

# 2. Define dataset
def return_prompt_and_responses(samples):

    return {
        "prompt": samples["prompt"] + "\n\n### Answer:",
        "chosen": samples["chosen"],
        "rejected": samples["rejected"],
    }
dataset = load_dataset(data_path)
train_dataset = dataset.map(return_prompt_and_responses)
train_dataset = train_dataset.filter(
    lambda x: len(x["prompt"]) + len(x["chosen"]) <= cutoff_len
    and len(x["prompt"]) + len(x["rejected"]) <= cutoff_len
)
train_dataset = train_dataset["train"].shuffle()

print(train_dataset['prompt'][0])
print(train_dataset['chosen'][0])
print(train_dataset['rejected'][0])

# 3. Define hyperparameters
training_args = TrainingArguments(
    num_train_epochs= num_epochs,
    per_device_train_batch_size=2,
    #per_device_eval_batch_size=script_args.per_device_eval_batch_size,
    #max_steps=1000,
    logging_steps=1,
    save_steps=10,
    save_total_limit=2,
    gradient_accumulation_steps=gradient_accumulation_steps,
    #gradient_checkpointing=script_args.gradient_checkpointing,
    learning_rate=learning_rate,
    #evaluation_strategy="steps",
    #eval_steps=script_args.eval_steps,
    output_dir=output_dir,
    #report_to=script_args.report_to,
    lr_scheduler_type=lr_scheduler,
    warmup_ratio=warmup_ratio,
    optim='adamw_bnb_8bit', # rmsprop
    bf16=True,
    remove_unused_columns=False,
    run_name="dpo_uine",
)

peft_config = LoraConfig(
    r=lora_r,
    lora_alpha=lora_alpha,
    lora_dropout=lora_dropout,
    target_modules=lora_target_modules,
    bias="none",
    task_type="CAUSAL_LM",
)

# DPO trainer
dpo_trainer = DPOTrainer(
    model,
    ref_model = model_ref, #model_ref,
    args=training_args,
    beta=0.1, # fix
    train_dataset=train_dataset,
    #eval_dataset=eval_dataset,
    tokenizer=tokenizer,
    peft_config=peft_config,
)

# init wandb
wandb.login(key="")  # 수정
combined_config = {**vars(training_args), **vars(peft_config)}
run = wandb.init(name = "", project="", config= combined_config)  # 수정

# train
dpo_trainer.train()

# finish wandb
wandb.finish()

# push to hub
dpo_trainer.push_to_hub()

"""## inference"""

# 런타임 연결 해제 후 다시 연결해서 모델 로드
!pip install -q accelerate==0.26.1 peft==0.8.2 bitsandbytes==0.42.0 transformers==4.37.2

import torch
from peft import AutoPeftModelForCausalLM
from transformers import (
    BitsAndBytesConfig,
    AutoTokenizer,
    TextStreamer,
    )

compute_dtype = getattr(torch, 'float16')

quant_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=compute_dtype,
    bnb_4bit_use_double_quant=False,
)


MODEL_DIR = "uine/single-dpo-practice-adapter"
model = AutoPeftModelForCausalLM.from_pretrained(MODEL_DIR,
                                                      quantization_config=quant_config,
                                                      device_map="auto")

tok = AutoTokenizer.from_pretrained("uine/single-dpo-practice-adapter", trust_remote_code=True)

streamer = TextStreamer(tok, skip_prompt=False, skip_special_tokens=False, device_map="auto")
s = "제가 요즘 너무 불안해요. 앞으로 뭐가 될지 모르겠어요."
conversation = [{'role': 'user', 'content': s}]
inputs = tok.apply_chat_template(
    conversation,
    tokenize=True,
    add_generation_prompt=True,
    return_tensors='pt').to("cuda")
_ = model.generate(inputs,
                   streamer=streamer,
                   max_new_tokens=1024,
                   use_cache=True,
                   repetition_penalty=1.2)

streamer = TextStreamer(tok, skip_prompt=False, skip_special_tokens=False, device_map="auto")
s ="""
요즘 스트레스가 많아서 잠을 잘 못자요.
스트레스가 많아 잠을 자지 못하신다니 정말 힘드시겠어요. 그 스트레스의 원인이 무엇인지 더 자세히 알려주실 수 있을까요? 업무 스트레스인지, 가족 문제인지, 아니면 다른 어떤 문제인지 궁금해요.
업무 스트레스 때문에 잠을 잘 못자는데, 어떻게 해야할까요?
업무 스트레스로 인해 잠을 제대로 자지 못하시는 것은 정말 고민거리일 것 같아요. 이 문제를 해결하기 위해 어떤 방법이 가장 효과적인지 알려주실 수 있으신가요? 혹시 스트레스 관리를 위해 운동이나 명상 같은 활동을 해보셨던 적이 있으신가요?
아직은 그런 시도를 해보지 못했어요. 어떤 운동이 효과적일까요?
"""
conversation = [{'role': 'user', 'content': s}]
inputs = tok.apply_chat_template(
    conversation,
    tokenize=True,
    add_generation_prompt=True,
    return_tensors='pt').to("cuda")
_ = model.generate(inputs,
                   streamer=streamer,
                   max_new_tokens=1024,
                   use_cache=True,
                   repetition_penalty=1.2)