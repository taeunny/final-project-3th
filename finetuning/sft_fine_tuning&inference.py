# -*- coding: utf-8 -*-
"""SFT_fine-tuning&inference.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nQPgmpVQipC-1IHtmH6Lp4N4P__PzK83

## SFT
"""

!pip install -q transformers datasets wandb accelerate==0.26.1 peft==0.8.2 bitsandbytes==0.42.0 transformers==4.37.2 trl==0.7.10

from datasets import load_dataset
import os
import torch
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    BitsAndBytesConfig,
    TrainingArguments,
    pipeline,
    logging,
)
from peft import (LoraConfig,
                  PeftModel,
                  AutoPeftModelForCausalLM)
from trl import SFTTrainer
from transformers import TextStreamer
import wandb

import huggingface_hub
huggingface_hub.login()

dataset = load_dataset("Smoked-Salmon-s/empathetic_dialogues_ko", split="train")

dataset

filtered_dataset = dataset.filter(lambda example: example['type'] == 'single')

def combine_texts(example):
    example['text'] ='Below is an instruction that describes a task, paired with an Input that provides further context. ' + 'Write a response that appropriately completes the request.\n\n' + '### Instruction:\n' + 'Answer based on context. You are a AI counselor chatbot like friend who empathizes, encourages, and helps person who is anxious or depressed. At the end of your answer, please ask question related to the context. You must complete your answer in three sentences. Be sure not to repeat the same answer.\n\n' + '### Input:\n'+ example['instruction'] + '\n\n' + '### Answer:\n' + example['output']
    return example

processed_dataset = filtered_dataset.map(combine_texts)

processed_dataset

my_dataset = processed_dataset.remove_columns(['instruction', 'output', 'source', 'type'])

print(my_dataset['text'][100])

my_dataset.push_to_hub("uine/single-practice-dataset")

dataset = load_dataset("uine/single-practice-dataset", split="train")

dataset

print(dataset['text'][100])

base_model = "yanolja/EEVE-Korean-Instruct-10.8B-v1.0"
compute_dtype = getattr(torch, "float16")

quant_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=compute_dtype,
    bnb_4bit_use_double_quant=False,
)

model = AutoModelForCausalLM.from_pretrained(
    base_model,
    quantization_config=quant_config,
    device_map="auto"
)

tokenizer = AutoTokenizer.from_pretrained("yanolja/EEVE-Korean-Instruct-10.8B-v1.0", trust_remote_code=True)
# tokenizer.pad_token = tokenizer.eos_token
# tokenizer.padding_side = "right"

peft_params = LoraConfig(
    lora_alpha=16,
    lora_dropout=0.1,
    r=64,
    bias="none",
    task_type="CAUSAL_LM",
)

# lora_alpha: scaling factor for the weight matrices. alpha is a scaling factor that adjusts the magnitude of the combined result (base model output + low-rank adaptation). We have set it to 16. You can find more details of this in the LoRA paper here.
# lora_dropout: dropout probability of the LoRA layers. This parameter is used to avoid overfitting. This technique basically drop-outs some of the neurons during both forward and backward propagation, this will help in removing dependency on a single unit of neurons. We are setting this to 0.1 (which is 10%), which means each neuron has a dropout chance of 10%.
# r: This is the dimension of the low-rank matrix, Refer to Part 1 of this blog for more details. In this case, we are setting this to 64 (which effectively means we will have 512x64 and 64x512 parameters in our LoRA adapter.
# bias: We will not be training the bias in this example, so we are setting that to “none”. If we have to train the biases, we can set this to “all”, or if we want to train only the LORA biases then we can use “lora_only”
# task_type: Since we are using the Causal language model, the task type we set to CAUSAL_LM.

training_params = TrainingArguments(
    output_dir="",  # 수정
    num_train_epochs=1,
    per_device_train_batch_size=8,
    gradient_accumulation_steps=2,
    optim="paged_adamw_32bit",
    save_steps=1000,
    logging_steps=100,
    learning_rate=2e-5,
    weight_decay=0.001,
    fp16=True,
    bf16=False,
    max_grad_norm=0.3,
    max_steps=-1,
    warmup_ratio=0.03,
    group_by_length=True,
    lr_scheduler_type="constant",
    report_to="wandb",
)

# output_dir: Output directory where the model predictions and checkpoints will be stored
# num_train_epochs=3: Number of training epochs
# per_device_train_batch_size=4: Batch size per GPU for training
# gradient_accumulation_steps=2: Number of update steps to accumulate the gradients for
# gradient_checkpointing=True: Enable gradient checkpointing. Gradient checkpointing is a technique used to reduce memory consumption during the training of deep neural networks, especially in situations where memory usage is a limiting factor. Gradient checkpointing selectively re-computes intermediate activations during the backward pass instead of storing them all, thus performing some extra computation to reduce memory usage.
# optim=”paged_adamw_32bit”: Optimizer to use, We will be using paged_adamw_32bit
# logging_steps=5: Log on to the console on the progress every 5 steps.
# save_strategy=”epoch”: save after every epoch
# learning_rate=2e-4: Learning rate
# weight_decay=0.001: Weight decay is a regularization technique used while training the models, to prevent overfitting by adding a penalty term to the loss function. Weight decay works by adding a term to the loss function that penalizes large values of the model’s weights.
# max_grad_norm=0.3: This parameter sets the maximum gradient norm for gradient clipping.
# warmup_ratio=0.03: The warm-up ratio is a value that determines what fraction of the total training steps or epochs will be used for the warm-up phase. In this case, we are setting it to 3%. Warm-up refers to a specific learning rate scheduling strategy that gradually increases the learning rate from its initial value to its full value over a certain number of training steps or epochs.
# lr_scheduler_type=”cosine”: Learning rate schedulers are used to adjust the learning rate dynamically during training to help improve convergence and model performance. We will be using the cosine type for the learning rate scheduler.
# report_to=”wandb”: We want to report our metrics to Weights and Bias

trainer = SFTTrainer(
    model=model,
    train_dataset=dataset,
    peft_config=peft_params,
    dataset_text_field="text",
    max_seq_length=None,
    tokenizer=tokenizer,
    args=training_params,
    packing=False,
)

wandb.login(key="")  # 수정
combined_config = {**vars(training_params), **vars(peft_params)}
run = wandb.init(name = "", project="", config= combined_config)  # 수정

trainer.train()

#stop reporting to wandb
wandb.finish()

trainer.push_to_hub()

"""## inference"""

# 런타임 연결 해제 후 다시 연결해서 모델 로드
!pip install -q accelerate==0.26.1 peft==0.8.2 bitsandbytes==0.42.0 transformers==4.37.2

import torch
from peft import AutoPeftModelForCausalLM
from transformers import (
    BitsAndBytesConfig,
    AutoTokenizer,
    TextStreamer,
    )

compute_dtype = getattr(torch, 'float16')

quant_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=compute_dtype,
    bnb_4bit_use_double_quant=False,
)


MODEL_DIR = "uine/single-practice-fine-tuning-eeve-adapter"
model = AutoPeftModelForCausalLM.from_pretrained(MODEL_DIR,
                                                      quantization_config=quant_config,
                                                      device_map="auto")

tok = AutoTokenizer.from_pretrained("uine/single-practice-fine-tuning-eeve-adapter", trust_remote_code=True)

streamer = TextStreamer(tok, skip_prompt=False, skip_special_tokens=False, device_map="auto")
s = "제가 요즘 너무 불안해요. 앞으로 뭐가 될지 모르겠어요."
conversation = [{'role': 'user', 'content': s}]
inputs = tok.apply_chat_template(
    conversation,
    tokenize=True,
    add_generation_prompt=True,
    return_tensors='pt').to("cuda")
_ = model.generate(inputs,
                   streamer=streamer,
                   max_new_tokens=1024,
                   use_cache=True,
                   repetition_penalty=1.2)

streamer = TextStreamer(tok, skip_prompt=False, skip_special_tokens=False, device_map="auto")
s ="""
요즘 스트레스가 많아서 잠을 잘 못자요.
스트레스가 많아 잠을 자지 못하신다니 정말 힘드시겠어요. 그 스트레스의 원인이 무엇인지 더 자세히 알려주실 수 있을까요? 업무 스트레스인지, 가족 문제인지, 아니면 다른 어떤 문제인지 궁금해요.
업무 스트레스 때문에 잠을 잘 못자는데, 어떻게 해야할까요?
업무 스트레스로 인해 잠을 제대로 자지 못하시는 것은 정말 고민거리일 것 같아요. 이 문제를 해결하기 위해 어떤 방법이 가장 효과적인지 알려주실 수 있으신가요? 혹시 스트레스 관리를 위해 운동이나 명상 같은 활동을 해보셨던 적이 있으신가요?
아직은 그런 시도를 해보지 못했어요. 어떤 운동이 효과적일까요?
"""
conversation = [{'role': 'user', 'content': s}]
inputs = tok.apply_chat_template(
    conversation,
    tokenize=True,
    add_generation_prompt=True,
    return_tensors='pt').to("cuda")
_ = model.generate(inputs,
                   streamer=streamer,
                   max_new_tokens=1024,
                   use_cache=True,
                   repetition_penalty=1.2)

