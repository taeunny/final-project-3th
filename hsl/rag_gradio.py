# -*- coding: utf-8 -*-
"""rag_gradio.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QOGFq3QRp8jek2LFrDn1RqM73nmkacCb
"""

from google.colab import drive
drive.mount('/content/drive')

# import torch

# if torch.cuda.is_available():

#   device_count = torch.cuda.device_count()
#   print("device_count: {}".format(device_count))

#   for device_num in range(device_count):
#     print("device {} capability {}".format(
#     device_num,
#     torch.cuda.get_device_capability(device_num)))
#     print("device {} name {}".format(
#     device_num, torch.cuda.get_device_name(device_num)))
# else:
#   print("no cuda device")

# import locale
# def getpreferredencoding(do_setlocale = True):
#     return "UTF-8"

!pip install -q accelerate==0.26.1 peft==0.8.2 bitsandbytes==0.42.0 transformers==4.37.2 langchain faiss-gpu sentence-transformers

!pip -q install gradio==3.45.0 --use-deprecated=legacy-resolver typing_extensions --upgrade

import torch
import time
import gradio as gr
from peft import (PeftModel,
                  AutoPeftModelForCausalLM)
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    BitsAndBytesConfig,
    TextStreamer
    )
# from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.embeddings.huggingface import HuggingFaceEmbeddings
from langchain.memory import VectorStoreRetrieverMemory
from langchain.chains import ConversationChain
from langchain.prompts import PromptTemplate
from langchain.docstore import InMemoryDocstore
from langchain.vectorstores import FAISS
from langchain.llms import HuggingFacePipeline
import faiss

compute_dtype = getattr(torch, 'float16')

quant_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=compute_dtype,
    bnb_4bit_use_double_quant=False,
)

peft_model = AutoPeftModelForCausalLM.from_pretrained("uine/practice-fine-tuning",
                                                      quantization_config=quant_config,
                                                      device_map={"": 0})

tokenizer = AutoTokenizer.from_pretrained("uine/practice-fine-tuning")

model_name = "jhgan/ko-sbert-nli"
encode_kwargs = {'normalize_embeddings': True}
hf = HuggingFaceEmbeddings(
    model_name=model_name,
    encode_kwargs=encode_kwargs
)

embedding_size = 768
index = faiss.IndexFlatL2(embedding_size)
embedding_fn = hf.embed_query
vectorstore = FAISS(embedding_fn, index, InMemoryDocstore({}), {})
retriever = vectorstore.as_retriever(search_kwargs=dict(k=2))
memory = VectorStoreRetrieverMemory(retriever=retriever, return_docs=False)

# streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)

# s = 'ë‚˜ ë„ˆë¬´ ìš°ìš¸í•´'
# history = memory.load_memory_variables({"query": s})["history"]
# ss=''

# for texts in history.split('\n'):
#   ss += texts + ' '

# context = ss + 'ì§ˆë¬¸: ' + s

# SYSTEM_PROMPT = """
# <s>[INST] <<SYS>>You are a counselor like friend who empathizes, encourages, and helps person who is anxious or depressed. Refer to the context of the conversation to answer current conversation. At the end of your answer, please ask questions related to the content so far. You must complete your answer in three sentences. Be sure not to repeat the same answer.<</SYS>>
# """
# conversation = [{'role': 'user', 'content': SYSTEM_PROMPT + context + '[/INST]'}]
# inputs = tokenizer.apply_chat_template(
# conversation,
# tokenize=True,
# add_generation_prompt=False,
# return_tensors='pt'
# ).to("cuda")
# _ = peft_model.generate(inputs, streamer=streamer, max_new_tokens=150, repetition_penalty=1.2)
# response = tokenizer.decode(_[0][len(inputs[0]):])

# memory.save_context(
#         {"ì§ˆë¬¸": s},
#         {"ë‹µë³€": response},
#     )

def res(message: str, history: list) -> str:

    history = memory.load_memory_variables({"query": message})["history"]
    ss=''

    for texts in history.split('\n'):
      ss += texts + ' '

    context = ss + 'ì§ˆë¬¸: ' + message

    SYSTEM_PROMPT = """
    <s>[INST] <<SYS>>You are a counselor like friend who empathizes, encourages, and helps person who is anxious or depressed. You must complete your answer in three sentences. Be sure not to repeat the same answer.<</SYS>>
    """

    # SYSTEM_PROMPT = """
    # <s>[INST] <<SYS>>You are a counselor like friend who empathizes, encourages, and helps person who is anxious or depressed. Refer to the context of the conversation to answer current conversation. At the end of your answer, please ask questions related to the content so far. You must complete your answer in three sentences. Be sure not to repeat the same answer.<</SYS>>
    # """

    streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)
    conversation = [{'role': 'user', 'content': SYSTEM_PROMPT + context + '[/INST]'}]
    inputs = tokenizer.apply_chat_template(
    conversation,
    tokenize=True,
    add_generation_prompt=False,
    return_tensors='pt'
    ).to("cuda")
    _ = peft_model.generate(inputs, streamer=streamer, max_new_tokens=250, repetition_penalty=1.2)
    response = tokenizer.decode(_[0][len(inputs[0]):])

    memory.save_context(
            {"ì§ˆë¬¸": message},
            {"ë‹µë³€": response},
        )

    print("ê³¼ê±° ëŒ€í™”:", ss, '\n')
    print("ì‚¬ìš©ì ì…ë ¥:", message, '\n')
    print("ë©˜í† ìŠ¤ ë‹µë³€:", response, '\n')

    for i in range(len(response)):
        time.sleep(0.03)
        yield response[:i+1]

gr.ChatInterface(
        fn=res,
        textbox=gr.Textbox(placeholder="ê³ ë¯¼ì„ ì–˜ê¸°í•´ì£¼ì„¸ìš”ğŸ™Œ", container=False, scale=1),
        title="ë©˜í† ìŠ¤(Mental Mate Talk on Support)",
        description="ë©˜í† ìŠ¤ëŠ” ë‹¹ì‹ ì˜ ê³ ë¯¼ì„ ë“¤ì–´ì£¼ë©° ê²©ë ¤í•´ì£¼ëŠ” ìƒë‹´ì¹œêµ¬ì—ìš”ğŸ˜Š",
        theme="soft",
        examples=[["ë‚˜ ìš°ìš¸í•´"], ["ë„ˆë¬´ ì§œì¦ë‚˜"], ["ì‚¬ëŠ”ê²Œ ì‰½ì§€ì•Šì•„"]],
        retry_btn="ë‹¤ì‹œë³´ë‚´ê¸° â†©",
        undo_btn="ì´ì „ì±— ì‚­ì œ âŒ",
        clear_btn="ì „ì±— ì‚­ì œ ğŸ’«"
).queue().launch(debug=True, share=True)

