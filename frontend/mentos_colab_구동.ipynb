{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "uMbqu3eD6t1q"
      },
      "outputs": [],
      "source": [
        "# A100 GPUì—ì„œ ì‹¤í–‰\n",
        "# êµ¬ê¸€ ë“œë¼ì´ë¸Œì— backend í´ë”ë¥¼ ì—…ë¡œë“œ í›„ ì§„í–‰\n",
        "import os\n",
        "os.chdir('/content/drive/MyDrive/final_project/backend')  # êµ¬ê¸€ ë“œë¼ì´ë¸Œ backend í´ë” ìœ„ì¹˜ë¥¼ ì…ë ¥"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qqq accelerate==0.26.1 peft==0.8.2 bitsandbytes==0.42.0 transformers==4.37.2 langchain faiss-gpu sentence-transformers"
      ],
      "metadata": {
        "id": "2FQsumJj9r2l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U langchain-community"
      ],
      "metadata": {
        "id": "eivQyOrKD6g3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -qqq install gradio==3.45.0 --use-deprecated=legacy-resolver typing_extensions --upgrade"
      ],
      "metadata": {
        "id": "bb5HFdY5babN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pymysql uvicorn pyngrok fastapi"
      ],
      "metadata": {
        "id": "ZhpczcMl9s18"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from peft import AutoPeftModelForCausalLM\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    )\n",
        "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
        "from langchain.memory import VectorStoreRetrieverMemory\n",
        "from langchain.memory import ConversationBufferWindowMemory\n",
        "from langchain.docstore import InMemoryDocstore\n",
        "from langchain.vectorstores import FAISS\n",
        "import faiss\n",
        "\n",
        "from transformers import ElectraTokenizer, ElectraForSequenceClassification, pipeline\n",
        "\n",
        "compute_dtype = getattr(torch, 'float16')\n",
        "\n",
        "quant_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=compute_dtype,\n",
        "    bnb_4bit_use_double_quant=False,\n",
        ")\n",
        "\n",
        "MODEL_DIR = \"hskhyl/EEVE-finetuned-05-13_1\"\n",
        "model = AutoPeftModelForCausalLM.from_pretrained(MODEL_DIR,\n",
        "                                                      quantization_config=quant_config,\n",
        "                                                      device_map=\"auto\")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"hskhyl/EEVE-finetuned-05-13_1\")\n",
        "\n",
        "model_name = \"jhgan/ko-sbert-nli\"\n",
        "encode_kwargs = {'normalize_embeddings': True}\n",
        "hf = HuggingFaceEmbeddings(\n",
        "    model_name=model_name,\n",
        "    encode_kwargs=encode_kwargs\n",
        ")\n",
        "\n",
        "embedding_size = 768\n",
        "index = faiss.IndexFlatL2(embedding_size)\n",
        "embedding_fn = hf.embed_query\n",
        "vectorstore = FAISS(embedding_fn, index, InMemoryDocstore({}), {})\n",
        "retriever = vectorstore.as_retriever(search_kwargs=dict(k=2))\n",
        "retriever_memory = VectorStoreRetrieverMemory(retriever=retriever, return_docs=False)\n",
        "ConversationBufferWindowMemory()\n",
        "buffer_memory = ConversationBufferWindowMemory(k=1, return_messages=False)"
      ],
      "metadata": {
        "id": "qDlHySLz-C1d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "compute_dtype = getattr(torch, 'float16')\n",
        "\n",
        "quant_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=compute_dtype,\n",
        "    bnb_4bit_use_double_quant=False,\n",
        ")\n",
        "\n",
        "MODEL_DIR = \"hskhyl/05-13_1-dpo\"\n",
        "model_dpo = AutoPeftModelForCausalLM.from_pretrained(MODEL_DIR,\n",
        "                                                      quantization_config=quant_config,\n",
        "                                                      device_map=\"auto\")"
      ],
      "metadata": {
        "id": "TX1jUoR2bkNc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = 0 if torch.cuda.is_available() else -1\n",
        "\n",
        "model_name = \"nlp04/korean_sentiment_analysis_kcelectra\"\n",
        "sentiment_analyzer = pipeline('sentiment-analysis', model=model_name, tokenizer=model_name, device=device)"
      ],
      "metadata": {
        "id": "eOl2jK5A-DiP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "device = 0 if torch.cuda.is_available() else -1\n",
        "converter_model = \"KoJLabs/bart-speech-style-converter\"\n",
        "speech_style_converter = pipeline('text2text-generation',model=converter_model, tokenizer=converter_model, device=device)"
      ],
      "metadata": {
        "id": "t3zGnHPB-JKN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import uvicorn\n",
        "from threading import Thread\n",
        "\n",
        "def run_server():\n",
        "    uvicorn.run(\"main:app\", host=\"0.0.0.0\", port=8000, log_level=\"info\")\n",
        "\n",
        "thread = Thread(target=run_server)\n",
        "thread.start()"
      ],
      "metadata": {
        "id": "5nk5Iaq396kP"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ìœ„ì— ì½”ë“œ ì‹¤í–‰ í›„, 3ë¶„ ì •ë„ í›„ì— ì•„ë˜ì½”ë“œ ì‹¤í–‰"
      ],
      "metadata": {
        "id": "SH_KgnYIdDfu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "ngrok.set_auth_token('')  # ngrok í† í° ì…ë ¥"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0yIwC9ID94r8",
        "outputId": "1dde8e25-fe2e-4b2b-e4f9-bde07e015586"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": []
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "public_url = ngrok.connect(8000)\n",
        "print(\"Public URL:\", public_url)\n",
        "# ì½”ë“œ ì‹¤í–‰ í›„, ì¶œë ¥ë˜ëŠ” ë‘ê°œì˜ ì£¼ì†Œ ì¤‘, ì™¼ìª½ ì²«ë²ˆì§¸ ì£¼ì†Œë¥¼ ë³µì‚¬í•´ì„œ, BACKEND_URLì— ì…ë ¥"
      ],
      "metadata": {
        "id": "S_mUfBsB97jF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ngrok_process = ngrok.get_ngrok_process()\n",
        "# try:\n",
        "#     # ngrok í”„ë¡œì„¸ìŠ¤ ë¡œê·¸ë¥¼ ì‹¤ì‹œê°„ìœ¼ë¡œ ì¶œë ¥\n",
        "#     ngrok_process.proc.wait()\n",
        "# except KeyboardInterrupt:\n",
        "#     print(\"Shutting down ngrok...\")\n",
        "#     ngrok.kill()"
      ],
      "metadata": {
        "id": "kCJr5T0R976t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import requests\n",
        "import gradio as gr\n",
        "BACKEND_URL = \"\"\n",
        "\n",
        "def res(message: str, history, tone: str) -> str:\n",
        "    payload = {\"msg\": message,\n",
        "               \"tone\": tone,\n",
        "               }\n",
        "    response = requests.post(\n",
        "        BACKEND_URL + \"/counselor\", data=json.dumps(payload)\n",
        "    ).json()\n",
        "    answer = response[\"result\"]\n",
        "    return answer\n",
        "\n",
        "demo = gr.ChatInterface(\n",
        "        fn=res,\n",
        "        textbox=gr.Textbox(placeholder=\"ê³ ë¯¼ì„ ì–˜ê¸°í•´ì£¼ì„¸ìš”ğŸ™Œ\", container=False, scale=1),\n",
        "        title=\"ë©˜í† ìŠ¤(Mental Mate Talk on Support)\",\n",
        "        description=\"ë©˜í† ìŠ¤ëŠ” ë‹¹ì‹ ì˜ ê³ ë¯¼ì„ ë“¤ì–´ì£¼ë©° ê²©ë ¤í•´ì£¼ëŠ” ìƒë‹´ì¹œêµ¬ì—ìš”ğŸ˜Š\",\n",
        "        theme=\"soft\",\n",
        "        submit_btn=\"ë³´ë‚´ê¸°\",\n",
        "        retry_btn=\"ë‹¤ì‹œ ë³´ë‚´ê¸° â†©\",\n",
        "        undo_btn=\"ì´ì „ ëŒ€í™” ì‚­ì œ âŒ\",\n",
        "        clear_btn=\"ì „ì²´ ëŒ€í™” ì‚­ì œ ğŸ’«\",\n",
        "        additional_inputs=\n",
        "            gr.Radio(choices=[\"ë©˜í† ìŠ¤\", \"ì „ë¬¸ì ì¸ ìƒë‹´ì‚¬\", \"ë¬¸ì–´ì²´\", \"ì•ˆë“œë¡œì´ë“œ\", \"ì•„ì¬\", \"entp\", \"í• ì•„ë²„ì§€\", \"ë‚˜ë£¨í† \", \"ì„ ë¹„\", \"ì†Œì‹¬í•œ\"], label=\"ë§íˆ¬ ì„ íƒ\", value=\"ë©˜í† ìŠ¤\"),\n",
        "        additional_inputs_accordion_name =\"ë§íˆ¬ë¥¼ ë³€ê²½í•˜ê³  ì‹¶ìœ¼ì‹œë©´ í´ë¦­í•´ì£¼ì„¸ìš”ğŸ˜€\",\n",
        "        )\n",
        "\n",
        "demo.queue().launch(debug=True, share=True)"
      ],
      "metadata": {
        "id": "G0bt1cIS-L6h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "28veVXwBbuTM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}