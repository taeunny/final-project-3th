{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "uMbqu3eD6t1q"
      },
      "outputs": [],
      "source": [
        "# A100 GPU에서 실행\n",
        "# 구글 드라이브에 backend 폴더를 업로드 후 진행\n",
        "import os\n",
        "os.chdir('/content/drive/MyDrive/final_project/backend')  # 구글 드라이브 backend 폴더 위치를 입력"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qqq accelerate==0.26.1 peft==0.8.2 bitsandbytes==0.42.0 transformers==4.37.2 langchain faiss-gpu sentence-transformers"
      ],
      "metadata": {
        "id": "2FQsumJj9r2l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U langchain-community"
      ],
      "metadata": {
        "id": "eivQyOrKD6g3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -qqq install gradio==3.45.0 --use-deprecated=legacy-resolver typing_extensions --upgrade"
      ],
      "metadata": {
        "id": "bb5HFdY5babN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pymysql uvicorn pyngrok fastapi"
      ],
      "metadata": {
        "id": "ZhpczcMl9s18"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from peft import AutoPeftModelForCausalLM\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    )\n",
        "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
        "from langchain.memory import VectorStoreRetrieverMemory\n",
        "from langchain.memory import ConversationBufferWindowMemory\n",
        "from langchain.docstore import InMemoryDocstore\n",
        "from langchain.vectorstores import FAISS\n",
        "import faiss\n",
        "\n",
        "from transformers import ElectraTokenizer, ElectraForSequenceClassification, pipeline\n",
        "\n",
        "compute_dtype = getattr(torch, 'float16')\n",
        "\n",
        "quant_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=compute_dtype,\n",
        "    bnb_4bit_use_double_quant=False,\n",
        ")\n",
        "\n",
        "MODEL_DIR = \"hskhyl/EEVE-finetuned-05-13_1\"\n",
        "model = AutoPeftModelForCausalLM.from_pretrained(MODEL_DIR,\n",
        "                                                      quantization_config=quant_config,\n",
        "                                                      device_map=\"auto\")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"hskhyl/EEVE-finetuned-05-13_1\")\n",
        "\n",
        "model_name = \"jhgan/ko-sbert-nli\"\n",
        "encode_kwargs = {'normalize_embeddings': True}\n",
        "hf = HuggingFaceEmbeddings(\n",
        "    model_name=model_name,\n",
        "    encode_kwargs=encode_kwargs\n",
        ")\n",
        "\n",
        "embedding_size = 768\n",
        "index = faiss.IndexFlatL2(embedding_size)\n",
        "embedding_fn = hf.embed_query\n",
        "vectorstore = FAISS(embedding_fn, index, InMemoryDocstore({}), {})\n",
        "retriever = vectorstore.as_retriever(search_kwargs=dict(k=2))\n",
        "retriever_memory = VectorStoreRetrieverMemory(retriever=retriever, return_docs=False)\n",
        "ConversationBufferWindowMemory()\n",
        "buffer_memory = ConversationBufferWindowMemory(k=1, return_messages=False)"
      ],
      "metadata": {
        "id": "qDlHySLz-C1d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "compute_dtype = getattr(torch, 'float16')\n",
        "\n",
        "quant_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=compute_dtype,\n",
        "    bnb_4bit_use_double_quant=False,\n",
        ")\n",
        "\n",
        "MODEL_DIR = \"hskhyl/05-13_1-dpo\"\n",
        "model_dpo = AutoPeftModelForCausalLM.from_pretrained(MODEL_DIR,\n",
        "                                                      quantization_config=quant_config,\n",
        "                                                      device_map=\"auto\")"
      ],
      "metadata": {
        "id": "TX1jUoR2bkNc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = 0 if torch.cuda.is_available() else -1\n",
        "\n",
        "model_name = \"nlp04/korean_sentiment_analysis_kcelectra\"\n",
        "sentiment_analyzer = pipeline('sentiment-analysis', model=model_name, tokenizer=model_name, device=device)"
      ],
      "metadata": {
        "id": "eOl2jK5A-DiP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "device = 0 if torch.cuda.is_available() else -1\n",
        "converter_model = \"KoJLabs/bart-speech-style-converter\"\n",
        "speech_style_converter = pipeline('text2text-generation',model=converter_model, tokenizer=converter_model, device=device)"
      ],
      "metadata": {
        "id": "t3zGnHPB-JKN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import uvicorn\n",
        "from threading import Thread\n",
        "\n",
        "def run_server():\n",
        "    uvicorn.run(\"main:app\", host=\"0.0.0.0\", port=8000, log_level=\"info\")\n",
        "\n",
        "thread = Thread(target=run_server)\n",
        "thread.start()"
      ],
      "metadata": {
        "id": "5nk5Iaq396kP"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 위에 코드 실행 후, 3분 정도 후에 아래코드 실행"
      ],
      "metadata": {
        "id": "SH_KgnYIdDfu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "ngrok.set_auth_token('')  # ngrok 토큰 입력"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0yIwC9ID94r8",
        "outputId": "1dde8e25-fe2e-4b2b-e4f9-bde07e015586"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": []
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "public_url = ngrok.connect(8000)\n",
        "print(\"Public URL:\", public_url)\n",
        "# 코드 실행 후, 출력되는 두개의 주소 중, 왼쪽 첫번째 주소를 복사해서, BACKEND_URL에 입력"
      ],
      "metadata": {
        "id": "S_mUfBsB97jF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ngrok_process = ngrok.get_ngrok_process()\n",
        "# try:\n",
        "#     # ngrok 프로세스 로그를 실시간으로 출력\n",
        "#     ngrok_process.proc.wait()\n",
        "# except KeyboardInterrupt:\n",
        "#     print(\"Shutting down ngrok...\")\n",
        "#     ngrok.kill()"
      ],
      "metadata": {
        "id": "kCJr5T0R976t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import requests\n",
        "import gradio as gr\n",
        "BACKEND_URL = \"\"\n",
        "\n",
        "def res(message: str, history, tone: str) -> str:\n",
        "    payload = {\"msg\": message,\n",
        "               \"tone\": tone,\n",
        "               }\n",
        "    response = requests.post(\n",
        "        BACKEND_URL + \"/counselor\", data=json.dumps(payload)\n",
        "    ).json()\n",
        "    answer = response[\"result\"]\n",
        "    return answer\n",
        "\n",
        "demo = gr.ChatInterface(\n",
        "        fn=res,\n",
        "        textbox=gr.Textbox(placeholder=\"고민을 얘기해주세요🙌\", container=False, scale=1),\n",
        "        title=\"멘토스(Mental Mate Talk on Support)\",\n",
        "        description=\"멘토스는 당신의 고민을 들어주며 격려해주는 상담친구에요😊\",\n",
        "        theme=\"soft\",\n",
        "        submit_btn=\"보내기\",\n",
        "        retry_btn=\"다시 보내기 ↩\",\n",
        "        undo_btn=\"이전 대화 삭제 ❌\",\n",
        "        clear_btn=\"전체 대화 삭제 💫\",\n",
        "        additional_inputs=\n",
        "            gr.Radio(choices=[\"멘토스\", \"전문적인 상담사\", \"문어체\", \"안드로이드\", \"아재\", \"entp\", \"할아버지\", \"나루토\", \"선비\", \"소심한\"], label=\"말투 선택\", value=\"멘토스\"),\n",
        "        additional_inputs_accordion_name =\"말투를 변경하고 싶으시면 클릭해주세요😀\",\n",
        "        )\n",
        "\n",
        "demo.queue().launch(debug=True, share=True)"
      ],
      "metadata": {
        "id": "G0bt1cIS-L6h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "28veVXwBbuTM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}